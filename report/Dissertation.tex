%You can delete all the comments after you have finished your document
%this sets up the defaults for the documents, 12pt font and A4 size. The article type sets this up as such as opposed to letter or memo.
%for the finer points LaTeX see https://en.wikibooks.org/wiki/LaTeX or http://tex.stackexchange.com/

\documentclass[12pt,a4paper]{article}
\usepackage{titlesec} %these are how we import packages, one helps set up footers and title layout
\usepackage{fancyhdr}

% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
%\usepackage[margin=3.5cm]{geometry}

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage[british]{babel}
\usepackage[style=apa,backend=biber]{biblatex}
\DeclareLanguageMapping{british}{british-apa}
\usepackage[toc,page]{appendix}
% These packages are all incorporated in the memoir class to one degree or another...

\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}

% MY PACKAGES
%\usepackage[table]{xcolor}
\usepackage{color, colortbl}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{multicol}
\usepackage{float}
\usepackage{blindtext}
\usepackage{longtable}
\usepackage{subcaption}
\usepackage{xr}

% EXTERNAL DOCUMENTS
\externaldocument{figures/kfoldfigure}
\externaldocument{figures/onehot}
\externaldocument{tables/kddattacks}
\externaldocument{tables/classifierconfig}


\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
  {-2.5ex\@plus -1ex \@minus -.25ex}%
  {1.25ex \@plus .25ex}%
  {\normalfont\normalsize\bfseries}}
\makeatother

\setcounter{secnumdepth}{4} % how many sectioning levels to assign numbers to
\setcounter{tocdepth}{4} % how many sectioning levels to show in ToC


%header and footer settings
\pagestyle{fancyplain}
\fancyhf{}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}
\setlength{\headheight}{15pt}
\fancyhead[L]{Jack Anderson - 40208539}
\fancyhead[R]{ SOC10101 Honours Project}
\fancyfoot[L]{}
\fancyfoot[C]{\thepage}

%set better section layout
\makeatletter
\renewcommand\subsection{\@startsection {subsection}{1}{2mm} % name, level, indent
      {3pt plus 2pt minus 1pt} % before skip
      {3pt plus 0pt} % after skip
      {\normalfont\bfseries}}
\makeatother
\makeatletter
\renewcommand\section{\@startsection {section}{1}{0mm} % name, level, indent
      {4pt plus 2pt minus 1pt} % before skip
      {4pt plus 0pt} % after skip
      {\bfseries}}
\makeatother

\addbibresource{bibliography.bib}

%this starts the document
\begin{document}

%you can import other documents into your main one, these layout the Title and Declarations on its own page.
%you might need to change these to \ if your on Microsoft Windows.
\input{./Dissertation-Title.tex}
\input{./Dissertation-Dec.tex}
\pagebreak
\input{./Dissertation-DP.tex}
\pagebreak

%LaTeX let you define the abstract separately so it wont get sucked into the main document.
\begin{abstract}
% fill the abstract in here
	For any major company or business, network security is of the utmost concern, as breaches can incur huge damages to infrastructure, data, and reputation, as well as large fines. When preventing these breaches the use of firewalls alone are insufficient and so network intrusion detection systems are put in place, which monitor incoming traffic and provide an extra layer of security. These intrusion detection systems can employ machine learning techniques which allow them to detect novel attacks and zero day vulnerabilities not previously seen. 
	
	While the existing literature on machine learning for network intrusion detection is extensive there is an apparent lack of comparisons between single stage and two-stage classification techniques, i.e. where when classifying traffic it is first categorised as normal or intrusive, and then classified as a specific attack. This project aims to fill the gap in the literature regarding the performance difference between single and two-stage classification of network intrusions, and to determine whether or not there is a increase in the accuracy of classifications made using two-stage classification.

	This dissertation presents a piece of software to simplify the comparison of single and two-stage classification techniques, and a comparison of several machine learning algorithms across the NSL-KDD and UNSW-NB15 datasets. Experimental results within this dissertation show that the overall accuracy of machine learning techniques can be improved through the use of two-stage classification and so is a worthwhile avenue of further research.


\end{abstract}
\pagebreak

\tableofcontents % is generated for you
\newpage

\listoftables
%generated in same way as figures
\newpage

\listoffigures
%you may have captions such as equations, listings etc they should all appear as required
%these are done for you as long as you use \begin{figure}[placement settings] .. bla bla ... \end{figure}
\newpage

\section*{Acknowledgements}
\noindent I would like the thank the following people without whom this dissertation would not have been possible: \\

\noindent Dr Simon Powers, for his excellent supervision and guidance throughout the duration of the project. \\

\noindent Dillon Hemphill and Dayna Craig, for their constant moral support. \\

\noindent The Games Lab Team, for keeping me productive and motivated with pomodoros and cheeky halves.



\newpage

\section{Introduction}
\subsection{Background}
	For any major company or business security is of the utmost concern, with a study finding that in the UK in 2016 an estimated 46\% of all businesses experienced a cyber security breach or attack \parencite{securitysurvey2017}. These breaches are particularly dangerous as even if a network is compromised a single time a company can have its entire database destroyed, or customer data leaked, leading to legal repercussions. When it comes to preventing these intrusions firewalls alone are insufficient for anything but the most rudimentary of attacks and so a Network Intrusion Detection System (NIDS) is employed to further bolster the security of the network. Traditional NIDS are placed strategically on a network to monitor all incoming traffic. It analyses the passing traffic and then compares it to a large library of known attacks and if it matches will flag the traffic. While these systems do provide some degree of protection they are unable to detect novel attacks or zero-day vulnerabilities and so some other method of identifying suspicious traffic is required. Introducing machine learning to a NIDS is one way of attempting to solve this problem first proposed by \cite{denning1987intrusion}. In using machine learning to detect network intrusions the system can be trained to recognise patterns of intrusive behaviour, allowing it to detect attacks which it may not have seen before but have characteristics of similar attacks. Machine learning also allows systems to be easily retrained to accommodate for new data on attacks as it emerges. There are two main categories of NIDS: misuse detection, and anomaly detection; both of which have their own advantages and disadvantages. The following literature review aims to discuss the different kinds of network intrusion detection systems, the algorithms that these systems employ, and the gap in papers which directly compare the performance of single stage and two-stage classifiers in the domain of network intrusion detection.
		
\subsection{Research Questions}
	For this honours thesis there are a number of research questions which have been collated, and an attempt made to answer them. These questions in which I am interested in answering are the following: 
		
		\begin{itemize}
			\item What is the rate of accurate detection and classification of network intrusions by single stage machine learning classification methods?
			\item What is the rate of accurate detection and classification of network intrusions by two stage machine learning classification methods?
			\item Which method of classification i.e. single stage or two stage, is more accurate and by what amount?
			\item Which configuration of algorithms in the two stage classifier produces the most accurate results?
		\end{itemize}
		
In this context, accuracy is defined as a high number of true positives and true negatives, and a low number of false positives and false negatives when classifying network intrusions. \\

	First research will be completed in order to gain an understanding of the history and current state of machine learning for network intrusion detection, through reading relevant research papers and articles. Next the individual algorithms and methods which go into detecting network intrusions will be researched and understood. This knowledge will then be put into developing a piece of software capable of running these algorithms and extracting metrics which will be used to answer these research questions.

\subsection{Aims and Objectives}
	The aim of this project is to create a piece of software which is capable of running both single and two stage classification algorithm arrangements on a number of datasets and displaying the results in a clear manner. In doing so this will assist in answering the research questions put forward.

	These research questions, specifically whether or not two stage classification provides more accurate results than single stage classification will be answered by meeting the following objectives:
\begin{itemize}
\item{Perform a review of existing literature.}
\item{Perform a review of existing software.}
\item{Select appropriate machine learning classification algorithms.}
\item{Select appropriate datasets for use in network intrusion detection.}
\item{Implementation of software to assist in comparison of classification algorithms.}
\item{Evaluation of software with regards to specification and other software.}
\item{Evaluation of single stage classification performance versus two-stage classification performance.}
\end{itemize}

\subsection{Scope and Limitations}
	\subsubsection{Deliverables}
	The list of deliverables for this project are the following:
	\begin{itemize}
	\item{A review of literature on related topics and a report detailing the findings of such reports.}
	\item{A review of existing software which performs a similar role to the proposed software.}
	\item{A software requirement specification.}
	\item{A description of all testing which will be carried out.}
	\item{A report detailing the results of each classification algorithm and configuration of two stage classification algorithms}
	\item{An evaluation of the implemented software with regards to how well it meets the proposed specification, and how it compares to existing software.}
	\end{itemize}
	\subsubsection{Boundaries}
	There exists a large number of different classification algorithms and methods to perform network intrusion detection, therefore this project must focus on a few specifically. This project will focus on performing misuse detection rather than anomaly detection, and will limit the classification algorithms used to: k-Nearest Neighbour, Multi-layer Perceptron, and a Support Vector Machine. This investigation will also only be assessing the performance of misuse detection, and not anomaly detection.
	\subsubsection{Constraints}
	The largest constraint facing this project is that of time. There is a large amount of literature to be reviewed, and also a considerable amount of results which must be collected and results drawn from in a short amount of time.
\subsection{Structure of this Dissertation}
This dissertation can be divided into three main parts: part one is a review of the relevant literature and related works carried out in the field of network intrusion detection, as well as any existing software; Part two details the planning, design, implementation and testing of the software which is to be created to aid in the investigation; finally, part three details the results of the experiments carried, an evaluation of the created software, an analysis of gathered results, and the conclusions drawn from the project as a whole. \\

Part one (Sections 2-3) involves a review of the existing literature within the field of machine learning for network intrusion detection and examines results of related works and individual machine learning algorithms. A review will also be done of existing software to see what features they offer in what aspects they might be improved. \\

Part two (Sections 4-5) describes the detailed software requirements, specific functionality which should be implemented, user interface designs, and a listing of what testing should take place. The methodology for the project is also detailed in this part which includes a description of the technologies and libraries used, the datasets used any the processing performed upon them, and finally how data will be collected and what metrics will be drawn from the results. \\

Part three (Sections 5-8) is a listing of results gathered in experiments carried out and an evaluation of the final software product against its initial design and any existing software. An analysis of the results gathered will then take place and conclusions drawn, as well as a review of research questions, learning outcomes, and suggestions for future work. also has an evaluation of the overall project how well it met its aims and suggestions for future work.

\input{Dissertation-LR}

\newpage
\section{Existing Software}
\subsection{WEKA}
Waikato Environment for Knowledge Analysis (WEKA) is a software suite designed to aid in data analysis and prediction modelling using machine learning techniques \cite{hall2009weka}. WEKA supports a number of tasks including: clustering, regresssion, classification, data and algorithmic visualization, feature selection, and data preprocessing. All of WEKA's functionality is available through the command line and also through the supplied user interface. Although WEKA does include a user interface it can be overwhelming to new users, and presents a sizeable learning curve to begin using due to the extensive functionaily that the software provides. Providing WEKA with custom classification algorithms is done through the use of .jar files. This requires the user to pre-compile and package created classifiers, which can make quick editing and addition of new classifiers difficult, whereas the proposed software will use python allowing for .py files to be used directly, allowing for changes to accomdated more quickly and files added more easily.

	The software created in this project, will aim to provide similar functionality as WEKA albeit with a highly reduced scope. The reasoning behind this being that the software will be very easy to use for the comparison of single and two stage classifiers without providing a large amount of functionality, to allow the software to be easy to pick up, user friendly and have a very specific use case.
\subsection{Splunk}
Splunk is a peice of software for collecting and analysing large amounts of real time machine generated data including network traffic \parencite{splunk}. Splunks main use case is application management, network security and analytics. Although Splunk may be used for the comparison of classification algorithms on specific datasets, it is not its main use and it does not provide a simple way to compare single and multiple stage classifiers. In addition, the extremely extensive amount of functionaliy it offers presents a massive learning curve and so it may be more beneficial to have a specifically tailored peice of software which can provide more focused functionality such as the software proposed in this project.Splunk is also not open source software, which may be of concern to some and does not allow for user modification of the software.
\newpage
\section{Software}
This section of the report will provide a high level description of the functionality and requirements of the software as well as describing the role of the user which will interact with the system. Detailed descriptions of functionality will also be included, and a list of hardware and software constraints, dependencies and assumptions made about the system and users. A detailed testing description will then be given outlining any and all unit testing and functional testing which will take place. Finally an evaluation will take place assessing how well the produced software meets its initial requirements and design.
\subsection{Overall Description}
This section will give an overview of the whole proposed system, and the basic functionality that it requires. It will also describe the users of the system as well as the dependencies and operating environment of the system.
\subsubsection{Product Functions}
The proposed software which will be developed is primarly to assist in the comparison of single stage and two-stage classifiers. The software will accept a dataset and supporting files in the form of: A training dataset, or a number of folds to use in k-fold cross-validation; A list of dataset features with thier data types; the number of runs to perform for stochastic classifiers; and optionally a list of categories to sort attacks into after classification. The user may also manually sort each of the features into one of three types, numeric, nominal, or binary. 

The user may also specify any number of classifier configurations. These classifier configurations consist of either one or two filepaths which point to a python file containing the definition for a classifier, which use the specified classifier interface. 

Once both the dataset and classifier configurations have been entered by the user, the classifiers will be run on the dataset and make predictions about either the testing dataset or training set folds and determine the class for each entry in the set. Once these preditions have been made, the results will be presented to the user. The results shown to the user come in the form of: precision, recall and f1-score metrics for each class, and each attack category; a confusing matrix, and then raw statistics regarding classification including true postives, true negatives, false postives, false negatives, etc. The user will also be shown a graph of any class they choose which will show the true positives, true negatives, false positives, and false negatives for each classifier configuration for the specified class, allowing for quick and easy comparison between configurations.


\subsubsection{User Characteristics}
This software is primarily focused towards researchers and students interested in comparing machine learning classification algorithms with a single stage against those with multiple stages. The software may be used by them in order to quickly make comparisons between different configurations of classifiers and to assess the effectiveness of these configurations. They will be able to enter their own datasets and classifiers and receive results in a standard format, and to view these results in a graph.
\subsubsection{Operating Environment}
In order to run the software the following requirements must be met:
\begin{itemize}
	\item{Python 3.6.4+ with the following packages:}
	\begin{itemize}
		\item{NumPy 1.14.0+}
		\item{Pandas 0.22.0+}
		\item{Pandas-ml 0.5.0+}
		\item{Scikit-learn 0.19.1+}
		\item{Scipy 1.0.0}
		\item{Matplotlib 2.1.1+}
		\item{PyQt 5.9.1+}
	\end{itemize}
\item{One of the following operating systems:}
	\begin{itemize}
		\item{Windows 7 (x86) or greater}
		\item{macOS 10.11 or greater}
		\item{Linux with X11}
	\end{itemize}
	\item{A system capable of running one of the above operating systems.}
\end{itemize}

\subsection{Specific Requirements}
\subsubsection{User Interface} \label{ui}
When the user launches the program they should be presented with the main window as shown in Figure \ref{mainwf}. The window consists of several main features, in the bottom left corner of the UI is the whee classifier configurations are specified. The buttons under each text box which say 'Select Classifier' may be used to open a file picker dialog and select a filepath. The 'Two Stage Classifier' checkbox can be checked which will turn the classifier configuration into a two stage classifier, and the second classifier box will be required. The tab containing the '+' symbol at the top of this section can be used to create new tabs which contain classifier configurations, which can be switched to by selecting one of the tabs along the top edge. The 'Run' button directly below this can be used to run the classifier configurations on the specified dataset.

\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=\textwidth, center]{figures/Mainwindow.pdf}}
\caption{Main Window Result Tab Wireframe}
\label{mainwf}
\end{figure}
The large section in the top right of the window is the area in which classification results will be displayed, and the tabs above can be used to access the graph view, seen in Figure \ref{graphwf}. In this tab the drop down can be used to select a class and the 'Select' button used to generate a graph of true postives, true negatives, false postives and false negatives for each classifier configurations for the specified class. In the small box in the bottom right of the window, the debug output from the program and classifiers will be shown. This output can then be cleared by pressing the 'Clear' button just below it. In the top left hand corner the 'File' button may be pressed which will present the user with a dropdown menu through which the user may create a new classifier; which will open a new file dialog prompting for a filepath and create a new classifier file; or exit the program entirely.

\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=\textwidth, center]{figures/mainwindowgraph.pdf}}
\caption{Main Window Graph Tab Wireframe}
\label{graphwf}
\end{figure}

Beneath the 'File' button there is a textbox which will display details regarding the dataset which has been selected. If the user wishes to specify a dataset, the 'Dataset' button underneath this can be pressed, and the user will be presented with the window seen in Figure \ref{datasetwf}.

\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=\textwidth, center]{figures/datasetwindow.pdf}}
\caption{Dataset Window Wireframe}
\label{datasetwf}
\end{figure}

In this window the user may specify a filepath for the training dataset, testing datset, dataset fields and attack categories by selecting the corresponding 'Select File' button underneath the label. Once the dataset fields have been selected they will populate the 'Nominal', 'Binary' and 'Numeric' columns. If there is no type specified within the file they will all be placed under the 'Numeric' column. The user may then manually set the field type by selecting the field name, and then either pressing one of the buttons labelled '$<$' or '$>$', or by right clicking and selecting the destination column. If the user wishes to use k-fold cross validation instead of a testing dataset they can check the 'k-Fold Cross Validation' checkbox and specify the number of folds in the 'Folds' spinner. They may also specify the number of stochastic classifier runs by entering a number into the spinner labelled 'Stochastic Classifier Runs'. Once the user has finished entering a dataset, they can press the 'Ok' button and the window will close. When this happens the textbox labelled 'Dataset info' in Figures \ref{mainwf} and \ref{graphwf} will be populated with information regarding the selected dataset.


\subsubsection{Functional Requirements} \label{reqspec}
\input{reqspec}
\subsection{Testing}
Testing will be carried out on the system to verify that the system is compliant with all requirements and works according to the specification. Unit tests will be written using the Python Unit Testing Framework to automatically carry out unit test to ensure there are no errors within the system, and that when changes are made the system no new errors are introduced. Manual testing will also be carried out by following the steps detailed below in Table \ref{testcasetabel}, in order to ensure that the system meets the specification and that no errors are present. Manual testing is necessary in order to fully test areas of the code which unit tests cannot cover, such as the user interface functionality.
\input{tables/testcase.tex}

\newpage
\section{Methodology}
\subsection{Implementation}
\subsubsection{PyQt}
PyQt is a set of Python bindings for the Qt application framework, which facilitates the writing of cross platform Qt applications which are capable of running on Windows, Mac, and Linux \parencite{pyqt5}.

PyQt was chosen as the framework of choice for creating the application GUI over other frameworks such as Java Swing for a number of reasons, the greatest of which being that it would allow all development to be carried out in Python as the libraries which were already selected for machine learning were written in Python. Writing the entire project in one language comes with extra benefits such as being able to have a single development environment, simplifying application design as there is no need for interfaces between each language, and having the ability to write all unit tests in a single language. PyQt also has powerful tools which aid in the design and development of user interfaces in the form of Qt Creator, which increased the speed of development substantially, as well as an extensive set of online documentation.

\subsubsection{Scikit-Learn}
Scikit-Learn is a Python library consisting of a set of tools for easily performing machine learning and data analysis \parencite{scikit}. The library consists of a large number or pre-implemented classification, and preprocessing algorithms, as well as others such as clustering, regression etc. These pre-implemented algorithms allowed for a much faster development cycle and also guaranteed the correctness of all algorithms which were used, again speeding up development time by removing the need for testing individual algorithms. Sci-kit also provides some basic preprocessing in the form of scaling of inputs so that they are all within a normalised range, as well as some reporting which can be used to quickly extract a lot of information in the form of confusion matrices and classification reports from the results.
	
	Some other libraries were considered for implementing the machine learning algorithms such as Tensorflow due to it's highly configurable nature and high performance due to having GPU acceleration, however the implementations of similar algorithms are considerably more complex using Tensorflow, and as time was a major factor in this project Scikit-learn was the preferable library. One final factor which influenced this choice is Scikit-learn's interoperability with Matplotlib allowing for graphs to be created quickly and easily.

\subsubsection{Pandas}
Pandas is a library which provides high performance, simple data structures and data analysis tools for Python \parencite{pandas}. Pandas provides to the programmer the DataFrame object type which allows for highly complex data manipulation to be carried out easily, such as data alignment and handling of missing data, dataset slicing, indexing and subsetting, merging and joining of multiple sets of data, etc. and is highly optimised for speed as the critical code within the library is written in C, allowing for much higher performance when compared to Python. This means that when working with very large datasets, such as the NSL-KDD and UNSW-NB15 which both have around 100,000 rows of data each, manipulations can be carried out quickly. The documentation for Pandas is also extensive and there exists a large online community of users which is indispensable during the learning of and development within the library.

\subsubsection{Matplotlib}
Matplotlib \parencite{matplotlib} is a Python 2D plotting library which produces high quality graphs and charts in both hardcopy and interactive formats. Matplotlib makes creating complex charts extremely simple as well as offering a massive number of charts, including, histograms, bar charts, scatterplots, confusion matrices, etc., with as little lines of code as possible. One large reason for selecting Matplotlib for use is its interoperability with the other libraries and frameworks which were selected. It works with Scikit-learn with very little effort and also contains custom widgets for use with PyQt which allows for extremely fast and easy integration of Matplotlib charts into the user interface of the software.

\subsubsection{Software}
The software which was written is described with a class diagram, seen in Appendix \ref{classdiagram} specifying each distinct class created along with its methods, properties and relationship with other classes, and with a use case diagram showing the general function of the software from the perspective of the user, as seen in Appendix \ref{usecase}. A description of all of the software classes functions is as follows.
\paragraph{MainWindow}
The MainWindow class is the class which is responsible for all of the logic behind the user interface of the main window. It inherits the QMainWindow class; a PyQt type which provides a main application window, and from Ui\_MainWindow, which provides the specification and initialisation of the user interface. The classes main function is handling user input, creating new user interface elements and windows, and allowing the user to view classification results.
\paragraph{DatasetWindow}
The DatasetWindow class is primarily responsible for defining what dataset the user wishes to use, to specify the data type of each column of said dataset. The window also allows users to choose between a testing dataset or k-fold cross validation and specify a number of folds, and to specify the number of runs a stochastic classifier should be run for. This class inherits from QDialog which provides the class with buttons allowing the user to accept or cancel the information they have input. It also inherits from Ui\_DatasetWindow which like Ui\_MainWindow provides the user interface for the window.
\paragraph{UIObject}
UIObject is an interface which is used by other classes which implement a user interface for some window. It is required as when a user interface is inherited by a window class a number of methods are expected to be available in order to create and render the user interface at runtime.
\paragraph{Classifier}
Classifier is an interface which any classification algorithm which is to be used must implement. It has one property, 'stochastic', which determines whether or not the classifier must be run multiple times, and a 'run' method which carries out the classification and returns the results.
\paragraph{QClfSelector}
QClfSelector is a class which inherits from QWidget, the base class of all Qt user interface objects, and who's main function is to act as a widget that can be included within MainWindow multiple times. It provides the functionality of accepting user classifier arrangements and then fetching and running those classifiers, using whatever method has been specified by the user, be it k-fold cross validation or using a test dataset, or running stochastic classifiers multiple times and gathering the results.
\paragraph{QBarChart}
QBarChart is a class which inherits from 'FigureCanvas', a Matplotlib class which interfaces with PyQt in order to create widget which can be displayed within a PyQt application. The class accepts a set of classification results and then constructs a Matplotlib chart which can be displayed in the MainWindow class for the user to view.
\paragraph{ErrorMessage}
The ErrorMessage Class is the class responsible for displaying an error message popup to users, in a standard manner and encapsulates boiler plate code required for creating and showing a message box. The class inherits from QMessageBox which is a PyQt class provides a modal dialog for informing the user.
\subsection{Datasets}
\subsubsection{NSL-KDD}
Within the KDD-NSL dataset, traffic is categorised into either normal network traffic for a military network or into one of the following four attack categories:
\begin{itemize}
\item{Denial of Service (DoS) - Attacker tries to prevent legitimate users from using a service.}
\item{Remote to Local (r2l) - Attacker does not have an account on the victim machine, hence tries to gain access.}
\item{User to Root (u2r) - Attacker has local access to the victim machine and tries to gain super user privileges.}
\item{Probe - Attacker tries to gain information about the target host.}
\end{itemize}
A complete listing of all attacks and their categories can been seen in Table \ref{nslkddattacksamples}, as well as the sample count of each category of attack in Table \ref{nslkddsamples}.
\input{tables/kddattacks}
\input{tables/kddbroad}

The dataset itself is also comprised of 41 features per connection recorded, a detailed listing of each and their type can be seen in Appendix \ref{kddfeatures}.

\subsubsection{UNSW-NB15}
Similarly, within the UNSW-NB15 dataset, as with the NSL-KDD dataset, traffic is also categorised into either normal network traffic for a military network or into one of the following seven attack categories:
\begin{itemize}
	\item{Fuzzers - Attempting to cause a program or network suspended by feeding it the randomly generated data. Analysis 2,677 It contains different attacks of port scan, spam and html files penetrations.}
	\item{Backdoors - A tecnique in which a system security mechanism is bypassed stealthily to access a computer or its data.}
	\item{DoS - A malicious attempt to make a server or a network resource unavailable to users, usually by temporarily interrupting or suspending the services of a host connected to the Internet.}
	\item{Exploits - The attacker knows of a security problem within an operating system or a piece of software and leverages that knowledge by exploiting the vulnerability.}
	\item{Generic - A technique works against all block-ciphers (with a given block and key size), without consideration about the structure of the block-cipher.}
	\item{Reconnaissance - Contains all Strikes that can simulate attacks that gather information.}
	\item{Shellcode - A small piece of code used as the payload in the exploitation of software vulnerability. Worms 174 Attacker replicates itself in order to spread to other computers. Often, it uses a computer network to spread itself, relying on security failures on the target computer to access it.}
\end{itemize}
\input{tables/unswattacks}
	The UNSW-NB15 dataset is comprised of 42 features per connection recorded, a detailed listing of each feature, their type and a description of each can be seen in Appendix \ref{unswfeatures}.
\subsection{Data Pre-processing}
Before classification can take place it is necessary to perform some pre-processing upon both datasets, so that when they are used they are in a form which is most useful for classification, allowing for more accurate, and faster classification. The first form of pre-processing which will take place upon the datasets is in the form of one hot encoding. One hot encoding is a process wherein categorical features of a dataset are converted into a binary representation. This process can be seen in Figure \ref{ohe}, which shows a small snippet from the NSL-KDD dataset before, and after one hot encoding.

\input{figures/onehot}

One hot encoding is necessary as many classification algorithms rely on either some kind of internal averaging, or some form of activation function which require numerical input in order to extract meaning from. Simply converting each of these categories from a nominal to a numeric representation is not sufficient as the categories lose meaning when represented by a series of numbers, i.e. when calculating distance from one category to another any number selected will be arbitrary, and the machine learning algorithm will attempt to extract a relationship between the numbers where there is none, therefore reducing accuracy in classification. 

One side effect of one hot encoding is that when processing both the training and testing dataset it is possible that the testing set is missing some categories present within the training set, leading to the algorithms being unable to perform a categorisation. To rectify this the next form of pre-processing which must take place is the insertion of missing features from the training set into the testing set. To do this the training set is simply scanned to find features which are not present in the testing set, and then inserts them into the testing set at the same index thereby making both sets features identical.

The final form of pre-processing to be performed on the dataset is feature scaling. Feature scaling is where a range of data is normalised by scaling it to be within some range, in this case between -1 and 1. The formula used for doing this is seen below in Figure \ref{scale}:
\begin{figure}[H]
	\[ x' = \frac{x - min(x)}{max(x) - min(x)} \] 
\caption{Feature Scaling Formula}
\label{scale}
\end{figure}
where $ x $ is the original value and $ x' $ is the normalised value. Representing a range of values between a normalised range is important as a large number of classification algoithms compute the euclidian distance between two points and will therefore not work correctly if two different featues have wildy different scales. For example if one feature has a large range of numbers and another a very narrow range, the distance will be dependant only on the feature with the larger range, so each feature should be normalised to ensure that each contributes the same amount to the final distance which is computed. Feature scaling also allows classifers which use gradient descent, such as the multi-layer perceptron, to converge much faster allowing for faster collection of results.
\subsection{Classifier Configurations}
The following is a listing of the parameters used in the running of each of the classifiers implemented within the scikit-learn library, with the descriptions of each taken from the official documentation:

\input{tables/classifierconfig}

\subsection{Single Stage Classification}
The first form of classification which will take place is single stage classification. In single stage classification only a single machine learning technique and stage is implemented in order to classify network traffic. The dataset is first read and preprocessed, and the entire set is used in order to train the machine learning classifier. One the classifier has been trained, the classifier will make predictions about the testing set, classifying it into normal traffic or any one of the attack categories for the dataset it was trained upon, e.g. the attacks seen in Table \ref{nslkddattacksamples}.

\subsection{Two Stage Classification}
In second form of classification to take place is two stage classification. In two stage classification, the classification processes is split into two separate stages, the first indentifying normal traffic, and the second identifying abnormal. Before the first stage is trained, the dataset must first be flattened, with all attack types being reduced to simply 'attack', resulting in a set of data labelled only 'normal' and 'attack'. This new dataset is then used to train the first classifier, and then predictions made about the testing set, consisting of only 'normal' and 'attack' classifications. The second classifier is now trained on the original, unflattened dataset with all of the normal traffic removed, resulting in a dataset consisting of only attacks. Once the classifier has been trained, the traffic which was classified as being an attack by the first classifier is then fed into the second classifier, and is then labelled as a specific attack. The total result is then the combination of the traffic identified as normal within the first classifier and the attacks from the second classifier. 

	Performing classification in this manner is expected to give more accurate results than a single stage classifier as it allows each stage to specialise, with the first only needing to differentiate between two classes, 'normal' and 'attack', and the second stage focussed on specific attack types, in theory increasing the overall rate of accurate detection by reducing domain size.

\subsection{Data Collection}
When performing data collection a number of things were done to ensure the greatest consistency within the results gathered, that comparisons between classification methods are valid, and that the results portray the real world performance of such a classifier as closely as possible. The first thing done to ensure validity of results gathered is that classifiers which are stochastic, i.e. are non-deterministic, are run for a total of 30 times and the results are averaged to reduce the amount of randomness within the results and lower the chance that a classifier may perform abnormally well or otherwise.

To best represent the real world performance of the classifiers, k-fold cross validation was used as described by \cite{refaeilzadeh2009cross}. In k-fold cross validation the training dataset is partitioned into $ k $ equal parts, with one part being used as the testing dataset the classifier will make predictions about, and the remaining $ k-1 $ parts used as the training set. This process is repeated $ k $ times with a different fold used each time as the testing set, a diagram of which can been seen in Figure \ref{kfold}. 

\input{figures/kfoldfigure.tex}

The results from all of these folds can then be combined and used to obtain a single result for the performance of the classification method. Using this method for testing the performance of classifier is preferable to the use of a testing set as the all of the data available is used as both training data and testing data, therefore giving a more accurate estimation of how well the classifier will generalize to any independent dataset, i.e. it gives a more accurate idea of how well the classifier will perform in practice.

The k value selected for the k-fold cross validation to be carried out in this investigation is $ k=10 $ as a study carried out by \cite{kohavi1995study} found ten-fold cross validation to be the most effective in producing accurate results. The same k value was used throughout for each classifier, as well as the using the same folds for gathering results without randomising to ensure that results collected were comparable.

Finally the same parameters were used for each of the classifier arrangements, as described in Table \ref{params}, to ensure that results across classifier arrangements were consistent with one and other.

\subsection{Metrics}
The following section describes the metrics which will be extracted from each of the classification results.

Precision is a measure of how many of the classifications made are relevant, the formula for which can be seen below in Figure \ref{precision}:
\begin{figure}[H]
\[ precision = \frac{tp}{tp+fp} \]
\caption{Precision Formula}
\label{precision}
\end{figure}
where $tp$ is the number of true positives, and $fp$ is the number of false positives. For example, if a classifier were to have a precision of 1.0 that would mean that only relevant traffic was classified, there were no false positives. This however does not tell us what percentage of the relative traffic was actually classified. To do this another metric must be used called recall.

Recall, also known as sensitivity, is a measure of what percentage of the total relevant records were classified. The formula for recall can be seen below in Figure \ref{recall}:
\begin{figure}[H]
\[ recall = \frac{tp}{tp+fn} \]
\caption{Recall Formula}
\label{recall}
\end{figure}
where $tp$ is the number of true positives, and $fn$ is the number of false negatives. For example, if a classifier were to have a recall of 1.0, that would mean that every single record of that class had been identified and classified correctly, however even with a recall of 1.0 there may be a large number of false positives which is why it is important to also take a measure of precision. To combine both of these metrics into a single value, f1-score can be used.

f1-Score is a metric which takes both the precision and recall into account, the formula for which can be seen below in Figure \ref{f1}:
\begin{figure}[H]
\[ f_1 = \frac{2tp}{2tp+fp+fn} \]
\caption{f1-Score Formula}
\label{f1}
\end{figure}

where $tp$ is the number of true positives, $fp$ the number of false positives, and $fn$ the number of false negatives. This metric is extremely useful if false positives and false negatives are weighted the same in terms of negative effects. Note that f1-score does not take true negatives into account, however in this case the sheer volume of true negatives for each class would cause a metric taking them into account yield no useful insight.

\newpage
\section{Results} \label{results}
\subsection{NSL-KDD}
The following section lists the results gathered running each classifier configuration on the NSL-KDD dataset. All of the metrics calculated using these results were first calculated per class, then combined and averaged for each category. This is done to ensure that classes with a lower number of samples are equally represented as the number of samples for each class within a category can vary greatly. The total number of true postives, true negatives, false postives, and false negatives for each category can be found in Appendix \ref{nslraw}.
\input{plots/precision}
\input{plots/recall}
\input{plots/f1score}
The values used in the creation of Figures \ref{nslprecision}, \ref{nslrecall}, and \ref{nslf1score} can be found in Appendices \ref{appendix:nslprecision}, \ref{appendix:nslrecall}, and \ref{appendix:nslf1score} respectively.
\subsection{UNSW-NB15}
The following section lists the results gathered running each classifier configuration on the UNSW-NB15 dataset. In this dataset, attacks are already grouped into categories and so no averaging is necessary. The total number of true positives, true negatives, false positives, and false negatives for each category can be found in Appendix \ref{unswraw}.
\input{plots/precisionUNSW}
\input{plots/recallUNSW}
\input{plots/f1scoreUNSW}
The values used in the creation of Figures \ref{unswprecision}, \ref{unswrecall}, and \ref{unswf1score} can be found in Appendices \ref{appendix:unswprecision}, \ref{appendix:unswrecall}, and \ref{appendix:unswf1score} respectively.
\newpage
\section{Evaulation}
\subsection{Software}
\subsubsection{User Interface}
From the wireframes shown in Section \ref{ui}, a user interface was created conforming to those designs which will be discussed below.

The main window in Figure \ref{mwrs}, shown below, was created from the design in Figure \ref{mainwf}. As can be seen in this figure, the user can select filepaths which are then inserted into the appropriate text boxes, and there is a button available above the classifier configurations which will create a new tab in which to specify a new configuration. It can also be seen that the main window contains a box in the top right of the window, showing details about the selected dataset, a debug log with accompanying clear button, and a run button under the classifier configurations used for running those configurations.
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/mainwindowresultscreenshot}
	\caption{Main Window Results Screenshot}
	\label{mwrs}
\end{figure}
The main box on the right hand side of the window contains the raw results for the classifier configurations, with different classifier results being available by selecting the corresponding number tab at the top of the box.
Below in Figure \ref{mwmbs}, the menu bar can be seen showing the expected functionality, namely, the ability to create new classifiers and to exit the program.
\begin{figure}[H]
	\centering
	\includegraphics[trim={0cm 15cm 15cm 0},clip,width=1.0\textwidth]{figures/menubarscreenshot}
	\caption{Main Window Menu Bar Screenshot}
	\label{mwmbs}
\end{figure}
Figure \ref{mws}, shows the 'Graph' tab within the main window. The classification label can be selected through the drop down and the top of the graph, and then shown by pressing the 'Show' button. The graph produced shows the true positive, true negative, false positive, and false negative values for the selected label of all classifier configurations, allowing for easy comparison between each classifier.
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/mainwindowscreenshot}
	\caption{Main Window Graph Screenshot}
	\label{mws}
\end{figure}
The final user interface created is that for the dataset window, shown in Figure \ref{dsws} and created from the wireframe in Figure \ref{datasetwf}. It can be seen that all of the dataset files can be selected and their filenames placed into the appropriate text boxes, as well as the three columns, 'Nominal', 'Binary' and 'Numeric' being populated with the columns labels file selected. There exists buttons labeled '$<$' and '$>$' on each side of the columns for moving these labels between columns. Figure \ref{rcs} shows the functionality for moving column labels through the right click context menu. The dataset window also allows the user to select between a testing set and k-fold cross validation, as well as the number of folds, and the number of runs a stochastic classifier should be averaged over.
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/datasetwindowscreenshot}
	\caption{Dataset Window Screenshot}
	\label{dsws}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[trim={17cm 15cm 7cm 1cm}, clip, width=0.7\textwidth]{figures/rcscreenshot}
	\caption{Right Click Context Menu}
	\label{rcs}
\end{figure}
From all of these figures, it can be seen that the produced software follows the wireframe designs created in Section \ref{ui}.
\subsubsection{Functional Requirements}
Out of all of the functionality listed in Section \ref{reqspec} only two pieces of functionality were not delivered, the counts of which can be seen in Figure \ref{reqgraph}, and the priorities of each functional requirement can be seen in the MoSCoW analysis in Appendix \ref{moscow}.
\input{plots/reqgraph}
The two pieces of functionality which were not delivered were FR21 Negative Selection Classifier, and FR24 Feature Selection. The negative selection classifier was ultimately not delivered due to time. This method of classification was not available within Sci-kit Learn or any libraries which could be found and implementing this classifier from scratch would have taken a considerable amount of time, as well as extensively testing it to ensure correctness. Another problem with this method of classification comes in the form of the time which is required in order to train it. Generating detectors is an expensive process, which is only made more expensive by the large number of features in the selected datasets, and by implementing the algorithm in Python, which is considerably slower than an implementation written in c, and while c and Python interoperability is possible through Cython, this was out of scope for the project. For these reasons it was decided to not deliver this functionality.

The second piece of functionality not delivered, feature selection, was dropped for being out of scope.
The software which was produced was primarily to be used for comparing classifier results. While implementing some kind of feature selection might have improved these classifier results and reduced the running times of the classifiers, it would have taken a large amount of time to create, and integrate with the user interface. Instead, it is assumed that when running the software, that the dataset will have already been feature selected, reducing the scope of the software and keeping it much more simple, accessible and user friendly.

\subsubsection{Non-Functional Requirements}
\paragraph{QR1 Robustness}
The software produced is shown to meet the requirment of being robust through both unit and manual testing. The software supports datasets of many feature sizes, and up to hundreds of thousands of rows, as well as datasets with any number of destination classes, and any number of categories which to sort classes into and aggregate results for. If there are errors present within a dataset or supplied classifier the software will present an error message, and allow the user to re-select the faulty dataset or classifier without crashing or becoming unresponsive.
\paragraph{QR2 Responsiveness}
User interface responsiveness was only partially achieved in the development of the software. When running one or multiple classifier configurations, the main thread will be blocked, only updating when debug output is written to the debug log at the bottom of the main window. This is not seen as a great problem however, as the running of classifiers cannot be canceled, and the debug output is still output in real time.
\paragraph{QR3 Usability}
When compared to existing software such as WEKA and Splunk, the produced software is much simpler, and consequently easier to learn and use. All aspects of the user interface are clearly laid out for the user to see, and it takes no more than three clicks to reach any piece of functionality from any state in the program, making navigation quick and simple. The software is also highly focussed with a small scope, only aiming to compare single and two stage classification configurations, unlike WEKA which aims to provide an extensive set of functionality so that users can carry out many different and complex use cases, in turn making the program much more user friendly.
\paragraph{QR4 Maintainability}
Maintainability will be aided in future through the use of comments throughout the program, describing the use of large methods and complex sections of code, sensible variable names which are self documenting. The class and use case diagram which were produced during the design of the software will also aid in the maintainance of the software, as well as the unit tests produced which can be used to ensure no functionality is broken when adding new features.
\paragraph{QR5 Portability}
The software produced is highly portable, which is facilitated by what libraries and technologies where used in the production of the software. The Python language can be run on any operating system, as well as any packages and libraries that it uses, and the PyQt framework which was used for creating the user interface is available for use on all operating systems.
\paragraph{QR6 Correctness}
To ensure that all results produced by the software were correct, libraries were used to implement the complex classification algorithms, as well as handling the creation and modification of the data structure used to hold the datasets. These libraries have already been highly tested and proven to produce correct results. Unit tests were also created to ensure that the custom methods implemented for manipulating and preprocessing data produced the correct results.

\subsection{Classifier Performance}
When examining the results collected in Section \ref{results} a number of observations may be made, and conclusions drawn. In both the NSL-KDD and UNSW-NB15 datasets we can see the performance of the single stage classifiers as the first three results in each graph. From these single stage results we can get an idea of the general performance of each classification technique. When looking at the results in both datasets and for all metrics, it can be seen that combining any classifier with a more performant classifier for the second stage will always produce more accurate results, likewise, combining a classifier with a less performant classifier for the second stage will always produce less accurate results.

To get the most accurate idea of the performance gain or loss when using two-stage classification, configurations which use the same classifier in both stages of classification should be examined, which removes the inherent difference in classifier performances from the results.

When looking at the results from the NSL-KDD dataset, it can be seen that of the two stage classifier configurations consisting of the same two classifiers, i.e. k-NN/k-NN, MLP/MLP, and SVM/SVM, Figure \ref{nslprecision} show that k-NN/k-NN and MLP/MLP both have a higher precision, while SVM/SVM has a lowered precision. The SVM/SVM configuration has a lower number of true positives for normal connections, as can be seen in Appendix \ref{nslraw}, in turn increasing the total number of false positives for all attack categories and reducing overall precision. This tells us that when identifying normal traffic using a support vector machine, it is important to also have each attack class present to identify the boundaries between intrusive and normal traffic. When looking at recall in Figure \ref{nslrecall}, it can be seen that all configurations, k-NN/k-NN, MLP/MLP, and SVM/SVM, have increased a higher recall score. This shows that separating classifiers into two stages results in less false negatives overall for attacks in all classifiers, due to the second classifier being able to specialise in detecting only attacks. The f1-score for each of these classifiers gives us a good idea of what increase or decrease in performance we can expect from the classifiers assuming that false negatives and false positives are weighted the same, i.e. both are equally detrimental to classification results. Figure \ref{nslf1score} shows that both k-NN/k-NN and MLP/MLP see an increase in f1-score, while SVM/SVM sees a reduction, even though the change in precision and recall are equal. This decrease in f1-score is due to thier being a greater total number of false positives than false negatives, and with the same weighting will decrease the f1-score more than the lower number of false negatives will increase it. All of the changes in these metrics can be seen below in Table \ref{metricnsl}

\begin{table}[H]
\centering
\caption{Change in Metrics From Single to Two-Stage Classification on NSL-KDD Dataset.}
\label{metricnsl}
\begin{tabular}{@{}lccc@{}}
\toprule
          & \Delta Precision & \Delta Recall & \Delta f1-Score \\ \midrule
k-NN/k-NN & $+0.07$     & $+0.14$  & $+0.09$    \\
MLP/MLP   & $+0.03$     & $+0.17$  & $+0.04$    \\
SVM/SVM   & $-0.02$     & $+0.02$  & $-0.04$    \\ \bottomrule
\end{tabular}
\end{table}

When looking at results gathered from the UNSW-NB15 dataset, it can be seen that of the two stage classifier configurations, both MLP/MLP and SVM/SVM experience a drop in precision, while k-NN/k-NN experiences a significant increase in precision. In both the MLP/MLP and SVM/SVM configurations a decrease in true positives for normal traffic can be observed, shown in Appendix \ref{unswraw}, as in the SVM/SVM configuration for the NSL-KDD dataset, contributing to a higher number of false positives within attack classes and resulting in a lower total precision, shown in Figure \ref{unswprecision}. This shows us that in the UNSW-NB15 dataset which is a low footprint dataset, individual attack types are crucial in identifying boundaries between normal and abnormal traffic. These two configurations however show an increased total recall score, shown in Figure \ref{unswrecall}. This shows us that without the normal traffic the second classifier may specialise and identify attacks much more easily, however the misidentified normal traffic from the first stage still leads to a lower precision. The k-NN/k-NN configuration shows the results to the contrary, with it having a significantly increased precision, but lower recall. This difference is highly likely due to the inherent differences between classification methods. Both the k-NN and k-NN/k-NN configurations show the same classification rates for normal traffic, as shown in Appendix \ref{unswraw}, however the removal of normal traffic allows the k-NN/k-NN classifier to produce less false positives amongst attack classes, but a lower recall overall. This discrepancy can be explained by attack classes with a high number of samples receiving the false negatives of the classes with low samples, thereby decreasing recall by a large amount, and the classes with a large sample size will not have their precision affected in a significant way, resulting in higher precision and lower recall overall as all classes are weighted the same. As with the NSL-KDD dataset results, we can look at the f1-score of the classifier configurations, shown in Figure \ref{unswf1score}, to give us a good idea of the increase or decrease in overall performance of the classifiers. From the results we can see that the k-NN/k-NN configuration experiences an increase in f1-score over its single stage counterpart, MLP/MLP, shows no difference and SVM/SVM shows a decrease in overall performance. Again, as was with the NSL-KDD dataset, the f1-score is affected by the total number of classifications made in each category and is not simply a sum of precision and recall. The total change in precision, recall, and f1-score from single to two-stage classification can be seen below in Figure \ref{metricunsw}.

\begin{table}[H]
\centering
\caption{Change in Metrics From Single to Two-Stage Classification on UNSW-NB15 Dataset.}
\label{metricunsw}
\begin{tabular}{@{}lccc@{}}
\toprule
          & \Delta Precision & \Delta Recall & \Delta f1-Score \\ \midrule
k-NN/k-NN & $+0.43$     & $-0.06$  & $+0.09$    \\
MLP/MLP   & $-0.04$     & $+0.03$  & $+0.00$    \\
SVM/SVM   & $-0.16$     & $+0.08$  & $-0.04$    \\ \bottomrule
\end{tabular}
\end{table}

From all of these observations we may now draw some conclusions. It is always beneficial to combine classifier with a more performant classifier for the second stage of classification, which may prove useful if a certain classification method is required for the first stage, or if the running time is an issue a less performant but faster classifier may be used in conjunction with a higher performance classifier, to have both low running times and higher accuracy. 


From the NSL-KDD f1-score results seen in Figure \ref{nslf1score} and Appendix \ref{appendix:nslf1score}, the highest performing classifier was a two-stage classifier, comprised of two multi-layer perceptrons, producing an f1-score increase of 0.04 over its single stage counterpart and an f1-score increase of 0.01 over the next best performing classifier. From the f1-score results of the second dataset, UNSW-NB15, seen in Figure \ref{unswf1score} and Appendix \ref{appendix:unswf1score}, the joint highest performing classifiers were the multi-layer perceptron, MLP, and its two-stage counterpart, MLP/MLP.


When comparing the performance of two stage classifier configurations using the same classifier in both stages, it can be seen that the use of two stage classification can increase the performance with regards to precision, recall and f1-score, as is evident in both Tables \ref{metricnsl} and \ref{metricunsw}. Although introducing a second classification stage does not always result in increased performance, this may be due to the classifiers used, and an improvement may be observed by using custom tuned classifier parameters for both the first and second stage, and so two stage classification should be considered when constructing a network intrusion detection system.

\newpage
\section{Conclusion}
\subsection{Research Questions}
\subsubsection*{What is the rate of accurate detection and classification of network intrusions by single stage machine learning classification methods?}

From the results collected using the NSL-KDD dataset, which can be found in Appendix \ref{nslraw}, and the metrics calculated from them, which can be seen in Figures \ref{nslprecision}, \ref{nslrecall}, and \ref{nslf1score}, as well as the results collected using the UNSW-NB15 dataset, which can be found in Appendix \ref{unswraw}, and the metrics calculated from them, which can be seen in Figures \ref{unswprecision}, \ref{unswrecall}, and \ref{unswf1score}, a table of results about single-stage classifier has been constructed which can be seen below in Table \ref{singlestagemetric}.

\begin{table}[H]
\centering
\caption{Single Stage Classifier Performance}
\label{singlestagemetric}
\begin{tabular}{@{}llccc@{}}
\toprule
Dataset & Classifier & Precision & Recall & f1-Score \\ \midrule
NSL-KDD   & k-NN & 3.64     & 3.62  & 3.61    \\
          & MLP  & 4.08     & 3.83  & 4.00    \\
	  & SVM  & 4.38     & 3.83  & 4.03    \\
UNSW-NB15 & k-NN & 2.93     & 3.20  & 3.00    \\
          & MLP  & 5.24     & 4.47  & 4.54    \\ 
	  & SVM  & 4.60     & 4.14  & 4.28    \\ \bottomrule
\end{tabular}
\end{table}


\subsubsection*{What is the rate of accurate detection and classification of network intrusions by two stage machine learning classification methods?}

Like the single stage classifiers, a table of two stage classifier metrics was created using results collected from both datasets found in Appendices \ref{nslraw}, and \ref{unswraw}, as well as Figures \ref{nslprecision}, \ref{nslrecall}, \ref{nslf1score}, \ref{unswprecision}, \ref{unswrecall}, and \ref{unswf1score} which can be seen below in Table \ref{twostagemetric}.

\begin{table}[H]
\centering
\caption{Two Stage Classifier Performance}
\label{twostagemetric}
\begin{tabular}{@{}llccc@{}}
\toprule
Dataset & Classifier & Precision & Recall & f1-Score \\ \midrule
NSL-KDD   & k-NN/k-NN & 3.71     & 3.76  & 3.70    \\
	  & k-NN/MLP & 3.72 & 3.67 & 3.68 \\
	  & k-NN/SVM & 4.1 & 3.62  & 3.73 \\
	  & MLP/k-NN & 3.63 & 3.57 & 3.52 \\
          & MLP/MLP   & 4.11     & 4.00  & 4.04    \\
	  & MLP/SVM & 4.42 & 3.83 & 4.02 \\
	  & SVM/k-NN & 3.45 & 3.65 & 3.49 \\
	  & SVM/MLP & 3.77 & 4.03 & 3.90 \\
	  & SVM/SVM   & 4.36     & 3.85  & 4.02    \\
UNSW-NB15 & k-NN/k-NN & 3.36     & 3.14  & 3.06    \\
	  & k-NN/MLP & 3.88 & 3.37 & 3.45 \\
	  & k-NN/SVM & 3.48 & 3.21 & 3.28 \\
	  & MLP/k-NN & 3.76 & 3.66 & 3.56 \\
          & MLP/MLP   & 5.20     & 4.50  & 4.54    \\ 
	  & MLP/SVM & 4.59 & 4.27 & 4.34 \\
	  & SVM/k-NN & 3.69 & 3.59 & 3.5 \\
	  & SVM/MLP & 5.07 & 4.4 & 4.43 \\
	  & SVM/SVM   & 4.44     & 4.18  & 4.24    \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection*{Which method of classification i.e. single stage or two stage, is more accurate and by what amount?}

From Tables \ref{metricnsl} and \ref{metricunsw}, we can see the difference between single stage and two stage classifier performance with regards to precision, recall, and f1-score. These tables do not include results of two stage classifiers comprising of two different classifiers. This is because when a classifier is combined with a more performant classifier for its second stage, it will always increase its performance, where as if a classifier is combined with a less performant classifier for its second stage it will always decrease its performance.

These results show that using two stage classification can improve classification performance in some classifiers, with further room for improvement perhaps being possible with tailored parameters for the classifiers in both the first and second stage.

\subsubsection*{Which configuration of algorithms in the two stage classifier produces the most accurate results?}

When using the NSL-KDD dataset the two stage configuration of classifiers which produces the most accurate results is the MLP/MLP configuration when examining f1-score. This configurations achieves an f1-score 0.01 higher than the next most performant configuration, and 0.04 higher than its single stage counterpart.

When using the UNSW-NB15 dataset the two stage configuration of classifiers which produces the most accurate results when looking at f1-score is also the MLP/MLP configuration. This configuration achieves the same performance as its single-stage counterpart and 0.2 higher f1-score than the next most performant classifier configuration.

\subsection{Has the Project met it's Aims and Objectives?}
The aim of this project was to create a piece of software to assist in the comparison of single and two-stage classifier configurations, on any arbitrary dataset and classification algorithm, and to use this created software to then gather results about those classifier configurations and compare them. This project has ultimately designed and managed the delivery of this software with almost all of the proposed functionality, as well as the results for multiple configurations of classifiers and a comparison of single and two-stage classification methods.
\subsubsection{Aims met}
\begin{itemize}
	\item{Perform a review of existing literature.}
	\item{Perform a review of existing software.}
	\item{Select appropriate machine learning classification algorithms.}
	\item{Select appropriate datasets for use in network intrusion detection.}
	\item{Implement software to assist in comparison of classification algorithms}
	\item{Evaluation of software with regards to specification and other software.}
	\item{Evaluation of single stage classification performance versus two-stage classification performance.}
\end{itemize}
\subsection{Reflection}
I feel as though the management of the project was handled well. At the beginning of the project a rough timeline was constructed using the aims, objectives and deliverables expected which was then revised during the week 9 interim meeting to better reflect the course of the project. Once the requirements were decided upon for the software to be created during the project, a MoSCoW analysis was carried out, seen in Appendix \ref{moscow}, which could be used to determine what the most important features were so that the project was delivered with the most functionality possible. Each week a meeting was held with my supervisor to ensure that the project was on track, and to decide upon the direction of the project, and what goals to complete in the following week. At the end of every meeting a diary sheet was compiled listing the work completed in the week prior and the goals set for completion in the following week, a complete listing of these diary sheets can be seen in Appendix \ref{diarysheets}. Project management could have been further improved through the use of a piece of project management software such a Trello to keep track of all functionality and deliverables.

When beginning this project I had a very minimal amount of experience using Python, and zero experience in network security or machine learning techniques, and so a large amount of reading, research and learning was required. I feel as though I have performed extremely well in learning the topic of network intrusion detection using machine learning, and also in assessing, selecting, and learning technologies quickly at the start of the project. The software which was delivered although I believe to be of a high quality could have been more feature rich, including some more dataset pre-processing options, and different options for displaying results so that classifier could be compared more easily.

I am pleased with the outcome of the tests and the results which were gathered, which shows that there is a potential benefit in the use of two stage classification, however a lot more work can be done to confirm the benefit of this approach. I think that if more consideration had been taken in the planning of the project and I had some domain knowledge prior to the project I could have performed a much more in depth comparison, with many more classifiers and datasets.

\subsection{Further Research}
Although this project has shown that there is improvements in classification performance to be made through the use of two-stage classification, there is a great deal of further investigation which can take place. Some proposed areas of further research are:

\begin{itemize}
	\item{ The use of a greater number of classifiers and configurations using methods such as: negative selection, self organising maps, random decision forests, decision trees, etc. in order to see how many classifiers benefit from being arranged in two stages.}
	\item{ The use of a greater number of datasets such as the PU-IDS or ADFA-Linux datasets, to see if similar results are observed across all datasets and two-stage classification is truly beneficial. }
	\item{To test and find the ideal parameters for each classifier in both the first stage, detecting normal and abnormal traffic, and in the second stage, classifying specific attacks, instead of having the same parameters for both stages, which would be highly likely to improve overall performance.}
	\item{An investigation into feature selection in conjunction with two-stage classification to see if performance can be improved more significantly by removing features with low relevance to classification.}
\end{itemize}

\newpage
	\printbibliography
\newpage

\begin{appendices}
\section{Project Overview}

%\input{ipo}
\includegraphics[page=1, width=\textwidth]{figures/ipo}
\newpage
\includegraphics[page=2, width=\textwidth]{figures/ipo}
\newpage
\includegraphics[page=3, width=\textwidth]{figures/ipo}
\newpage

\begin{subappendices}
\subsection{Project Timeline}\label{ganttchart}

\begin{figure}[H]
	\centering
	\includegraphics[width=1.31\textwidth,angle=90]{figures/Ganttchart}
	\caption{Project Timeline Gantt Chart}
	\label{gantt}
\end{figure}

\end{subappendices}

\newpage

\section{Second Formal Review Output}
\begin{center}
\includegraphics[page=1, width=\textwidth, center]{figures/interim}
\newpage
\includegraphics[page=2, width=\textwidth, center]{figures/interim}
\end{center}
\newpage

\section{Diary Sheets}\label{diarysheets}

\begin{longtable}{@{}cc@{}}
\includegraphics[page=1, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=2, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=3, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=4, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=5, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=6, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=7, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=8, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=9, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=10, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=11, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=12, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=13, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=14, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=15, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=16, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=17, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=18, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=19, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=20, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=21, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=22, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=23, width=0.5\textwidth]{figures/diaries} &
\end{longtable}


\newpage

\section{MoSCoW Analysis} \label{moscow}
\input{tables/moscow}

\newpage

\section{Class Diagram} \label{classdiagram}
\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth, center]{figures/classuml.pdf}}
\caption{UML Class Diagram}
\end{figure}

\newpage

\section{Use Case Diagram} \label{usecase}
\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth, center]{figures/usecase.pdf}}
\caption{UML Use Case Diagram}
\end{figure}

\newpage

\section{NSL-KDD Features} \label{kddfeatures}
\input{tables/nslkddfeatures}

\newpage

\section{UNSW-NB15 Features} \label{unswfeatures}

\input{tables/unswdescription}

\newpage

\section{NSL-KDD Results} 
\subsection{NSL-KDD Classifcation Data} \label{nslraw}
\input{tables/nslraw.tex}
\newpage

\subsection{NSL-KDD Precision} \label{appendix:nslprecision}
\input{tables/nslprecision}
\newpage
\subsection{NSL-KDD Recall} \label{appendix:nslrecall}
\input{tables/nslrecall}
\newpage
\subsection{NSL-KDD f1-Score} \label{appendix:nslf1score}
\input{tables/nslf1score}
\newpage

\section{UNSW-NB15 Results}
\subsection{UNSW-NB15 Classification Data} \label{unswraw}
\input{tables/UNSWraw}
\newpage

\subsection{UNSW-NB15 Precision} \label{appendix:unswprecision}
\input{tables/UNSWpecision}
\newpage

\subsection{UNSW-NB15 Recall} \label{appendix:unswrecall}
\input{tables/UNSWrecall}
\newpage

\subsection{UNSW-NB15 f1-Score} \label{appendix:unswf1score}
\input{tables/UNSWf1score.tex}
\newpage


\end{appendices}

\end{document}
