%You can delete all the comments after you have finished your document
%this sets up the defaults for the documents, 12pt font and A4 size. The article type sets this up as such as opposed to letter or memo.
%for the finer points LaTeX see https://en.wikibooks.org/wiki/LaTeX or http://tex.stackexchange.com/

\documentclass[12pt,a4paper]{article}
\usepackage{titlesec} %these are how we import packages, one helps set up footers and title layout
\usepackage{fancyhdr}

% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
%\usepackage[margin=3.5cm]{geometry}

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage[british]{babel}
\usepackage[style=apa,backend=biber]{biblatex}
\DeclareLanguageMapping{british}{british-apa}
\usepackage[toc,page]{appendix}
% These packages are all incorporated in the memoir class to one degree or another...

\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}

% MY PACKAGES
%\usepackage[table]{xcolor}
\usepackage{color, colortbl}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{multicol}
\usepackage{float}
\usepackage{blindtext}
\usepackage{longtable}
\usepackage{subcaption}
\usepackage{xr}

% EXTERNAL DOCUMENTS
\externaldocument{figures/kfoldfigure}
\externaldocument{figures/onehot}
\externaldocument{tables/kddattacks}
\externaldocument{tables/classifierconfig}


\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
  {-2.5ex\@plus -1ex \@minus -.25ex}%
  {1.25ex \@plus .25ex}%
  {\normalfont\normalsize\bfseries}}
\makeatother

\setcounter{secnumdepth}{4} % how many sectioning levels to assign numbers to
\setcounter{tocdepth}{4} % how many sectioning levels to show in ToC


%header and footer settings
\pagestyle{fancyplain}
\fancyhf{}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}
\setlength{\headheight}{15pt}
\fancyhead[L]{Jack Anderson - 40208539}
\fancyhead[R]{ SOC10101 Honours Project}
\fancyfoot[L]{}
\fancyfoot[C]{\thepage}

%set better section layout
\makeatletter
\renewcommand\subsection{\@startsection {subsection}{1}{2mm} % name, level, indent
      {3pt plus 2pt minus 1pt} % before skip
      {3pt plus 0pt} % after skip
      {\normalfont\bfseries}}
\makeatother
\makeatletter
\renewcommand\section{\@startsection {section}{1}{0mm} % name, level, indent
      {4pt plus 2pt minus 1pt} % before skip
      {4pt plus 0pt} % after skip
      {\bfseries}}
\makeatother

\addbibresource{bibliography.bib}

%this starts the document
\begin{document}

%you can import other documents into your main one, these layout the Title and Declarations on its own page.
%you might need to change these to \ if your on Microsoft Windows.
\input{./Dissertation-Title.tex}
\input{./Dissertation-Dec.tex}
\pagebreak
\input{./Dissertation-DP.tex}
\pagebreak

%LaTeX let you define the abstract separately so it wont get sucked into the main document.
\begin{abstract}
% fill the abstract in here
\end{abstract}
\pagebreak

\tableofcontents % is generated for you
\newpage

\listoftables
%generated in same way as figures
\newpage

\listoffigures
%you may have captions such as equations, listings etc they should all appear as required
%these are done for you as long as you use \begin{figure}[placement settings] .. bla bla ... \end{figure}
\newpage

\section*{Acknowledgements}
Insert acknowledgements here
\newpage

\section{Introduction}
\subsection{Background}
	For any major company or business security is of the utmost concern, with a study finding that in the UK in 2016 an estimated 46\% of all businesses experienced a cyber security breach or attack \cite{securitysurvey2017}. These breaches are particularly dangerous as even if a network is compromised a single time a company can have its entire database destroyed, or customer data leaked, leading to legal repercussions. When it comes to preventing these intrusions firewalls alone are insufficient for anything but the most rudimentary of attacks and so a Network Intrusion Detection System (NIDS) is employed to further bolster the security of the network. Traditional NIDS are placed strategically on a network to monitor all incoming traffic. It analyses the passing traffic and then compares it to a large library of known attacks and if it matches will flag the traffic. While these systems do provide some degree of protection they are unable to detect novel attacks or zero-day vulnerabilities and so some other method of identifying suspicious traffic is required. Introducing machine learning to a NIDS is one way of attempting to solve this problem first proposed by \cite{denning1987intrusion}. In using machine learning to detect network intrusions the system can be trained to recognise patterns of intrusive behaviour, allowing it to detect attacks which it may not have seen before but have characteristics of similar attacks. Machine learning also allows systems to be easily retrained to accommodate for new data on attacks as it emerges. There are two main categories of NIDS: misuse detection, and anomaly detection; both of which have their own advantages and disadvantages. This literary review aims to discuss the different kinds of network intrusion detection systems, the algorithms that these systems employ, and the gap in papers which directly compare the performance of single stage and two-stage classifiers in the domain of network intrusion detection.
		
\subsection{Research Questions}
	For this honours thesis there are a number of research questions which have been collated, and an attempt made to answer them. These questions in which I am interested in answering are the following: 
		
		\begin{itemize}
			\item What is the rate of accurate detection and classification of network intrusions by single stage machine learning classification methods?
			\item What is the rate of accurate detection and classification of network intrusions by two stage machine learning classification methods?
			\item Which method of classification i.e. single stage or two stage, is more accurate and by what amount?
			\item Which configuration of algorithms in the two stage classifier produces the most accurate results?
		\end{itemize}
		
In this context, accuracy is defined as a high number of true positives and true negatives, and a low number of false positives and false negatives when classifying network intrusions. \\

	First research will be completed in order to gain an understanding of the history and current state of machine learning for network intrusion detection, through reading relevant research papers and articles. Next the individual algorithms and methods which go into detecting network intrusions will be researched and understood. This knowledge will then be put into developing a piece of software capable of running these algorithms and extracting metrics which will be used to answer these research questions.

\subsection{Aims and Objectives}
	The aim of this project is to create a piece of software which is capable of running both single and two stage classification algorithm arrangements on a number of datasets and displaying the results in a clear manner. In doing so this will assist in answering the research questions put forward.

	These research questions, specifically whether or not two stage classification provides more accurate results than single stage classification will be answered by meeting the following objectives:
\begin{itemize}
\item{Perform a review of existing literature.}
\item{Perform a review of existing software.}
\item{Select appropriate machine learning classification algorithms.}
\item{Select appropriate datasets for use in network intrusion detection.}
\item{Implementation of software to assist in comparison of classification algorithms.}
\item{Evaluation of software with regards to specification and other software.}
\item{Evaluation of classification algorithm performance.}
\end{itemize}

\subsection{Scope and Limitations}
	\subsubsection{Deliverables}
	The list of deliverables for this project are the following:
	\begin{itemize}
	\item{A review of literature on related topics and a report detailing the findings of such reports.}
	\item{A review of existing software which performs a similar role to the proposed software.}
	\item{A software requirement specification.}
	\item{A description of all testing which will be carried out.}
	\item{A report detailing the results of each classification algorithm and configuration of two stage classification algorithms}
	\item{An evaluation of the implemented software with regards to how well it meets the proposed specification, and how it compares to existing software.}
	\end{itemize}
	\subsubsection{Boundaries}
	There exists a large number of different classification algorithms and methods to perform network intrusion detection, therefore this project must focus on a few specifically. This project will focus on performing misuse detection rather than anomaly detection, and will limit the classification algorithms used to: k-Nearest Neighbour, Multi-layer Perceptron, and a Support Vector Machine.
	\subsubsection{Constraints}
	The largest constraint facing this project is that of time. There is a large amount of literature to be reviewed, and also a considerable amount of results which must be collected and results drawn from in a short amount of time.
	Time
	Knowledge
	Misuse detection only
\subsection{Structure of this Dissertation}
This dissertation can be divided into three main parts. Part one is literature review and past projects within the field, and existing software. Second part details the planning design and implementation of the software and the third consists of an analysis and evaluation of results and software.

part one reviews the literature on the subject and examines results of past projects and individual algorithms, also reviews existing software for machine learning comparison.

part two describes the detailed software requirements and specific functionality which should be implemented. Also includes a description of technologies and libraries used, and a testing plan. 

part three is an evaluation of the final software product against its design. Analysis and evaluation of classifier results. also has an evaluation of the overall project how well it met its aims and suggestions for future work.

\input{Dissertation-LR}

\section{Existing Software}
\subsection{WEKA}
\subsection{SPLUNK}
\section{Software}
This section of the report will provide a high level description of the functionality and requirements of the software as well as describing the role of the user which will interact with the system. Detailed descriptions of functionality will also be included, and a list of hardware and software constraints, dependencies and assumptions made about the system and users. A detailed testing description will then be given outlining any and all unit testing and functional testing which will take place. Finally an evaluation will take place assessing how well the produced software meets its initial requirements and design.
\subsection{Overall Description}
\subsubsection{Product Perspective}
\subsubsection{Product Functions}
\subsubsection{User Characteristics}
This software is primarily focused towards researchers and students interested in comparing machine learning classification algorithms with a single stage against those with multiple stages. The software may be used by them in order to quickly make comparisons between different configurations of classifiers and to assess the effectiveness of these configurations. They will be able to enter their own datasets and classifiers and receive results in a standard format, and to view these results in a graph.
\subsubsection{Operating Environment}
In order to run the software the following requirements must be met:
\begin{itemize}
	\item{Python 3.6.4+ with the following packages:}
	\begin{itemize}
		\item{NumPy 1.14.0+}
		\item{Pandas 0.22.0+}
		\item{Pandas-ml 0.5.0+}
		\item{Scikit-learn 0.19.1+}
		\item{Scipy 1.0.0}
		\item{Matplotlib 2.1.1+}
		\item{PyQt 5.9.1+}
	\end{itemize}
\item{One of the following operating systems:}
	\begin{itemize}
		\item{Windows 7 (x86) or greater}
		\item{macOS 10.11 or greater}
		\item{Linux with X11}
	\end{itemize}
	\item{A system capable of running one of the above operating systems.}
\end{itemize}

\subsection{Specific Requirements}
\subsubsection{User Interface}
\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=\textwidth, center]{figures/Mainwindow.pdf}}
\caption{Main Window Result Tab Wireframe}
\label{mainwf}
\end{figure}
\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=\textwidth, center]{figures/mainwindowgraph.pdf}}
\caption{Main Window Graph Tab Wireframe}
\label{graphwf}
\end{figure}
\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=\textwidth, center]{figures/datasetwindow.pdf}}
\caption{Dataset Window Wireframe}
\label{datasetwf}
\end{figure}

\subsubsection{Functional Requirements}
\input{reqspec}
\subsection{Testing}
Paragraph about testing, unit testing and why
\input{tables/testcase.tex}

\section{Methodology}
\subsection{Implementation}
\subsubsection{PyQt}
PyQt is a set of Python bindings for the Qt application framework, which facilitates the writing of cross platform Qt applications which are capable of running on Windows, Mac, and Linux \parencite{pyqt5}.

PyQt was chosen as the framework of choice for creating the application GUI over other frameworks such as Java Swing for a number of reasons, the greatest of which being that it would allow all development to be carried out in Python as the libraries which were already selected for machine learning were written in Python. Writing the entire project in one language comes with extra benefits such as being able to have a single development environment, simplifying application design as there is no need for interfaces between each language, and having the ability to write all unit tests in a single language. PyQt also has powerful tools which aid in the design and development of user interfaces in the form of Qt Creator, which increased the speed of development substantially, as well as an extensive set of online documentation.

\subsubsection{Scikit-Learn}
Scikit-Learn is a Python library consisting of a set of tools for easily performing machine learning and data analysis \parencite{scikit}. The library consists of a large number or pre-implemented classification, and preprocessing algorithms, as well as others such as clustering, regression etc. These pre-implemented algorithms allowed for a much faster development cycle and also guaranteed the correctness of all algorithms which were used, again speeding up development time by removing the need for testing individual algorithms. Sci-kit also provides some basic preprocessing in the form of scaling of inputs so that they are all within a normalised range, as well as some reporting which can be used to quickly extract a lot of information in the form of confusion matrices and classification reports from the results.
	
	Some other libraries were considered for implementing the machine learning algorithms such as Tensorflow due to it's highly configurable nature and high performance due to having GPU acceleration, however the implementations of similar algorithms are considerably more complex using Tensorflow, and as time was a major factor in this project Scikit-learn was the preferable library. One final factor which influenced this choice is Scikit-learn's interoperability with Matplotlib allowing for graphs to be created quickly and easily.

\subsubsection{Pandas}
Pandas is a library which provides high performance, simple data structures and data analysis tools for Python \parencite{pandas}. Pandas provides to the programmer the DataFrame object type which allows for highly complex data manipulation to be carried out easily, such as data alignment and handling of missing data, dataset slicing, indexing and subsetting, merging and joining of multiple sets of data, etc. and is highly optimised for speed as the critical code within the library is written in C, allowing for much higher performance when compared to Python. This means that when working with very large datasets, such as the NSL-KDD and UNSW-NB15 which both have around 100,000 rows of data each, manipulations can be carried out quickly. The documentation for Pandas is also extensive and there exists a large online community of users which is indispensable during the learning of and development within the library.

\subsubsection{Matplotlib}
Matplotlib \parencite{matplotlib} is a Python 2D plotting library which produces high quality graphs and charts in both hardcopy and interactive formats. Matplotlib makes creating complex charts extremely simple as well as offering a massive number of charts, including, histograms, bar charts, scatterplots, confusion matrices, etc., with as little lines of code as possible. One large reason for selecting Matplotlib for use is its interoperability with the other libraries and frameworks which were selected. It works with Scikit-learn with very little effort and also contains custom widgets for use with PyQt which allows for extremely fast and easy integration of Matplotlib charts into the user interface of the software.

\subsubsection{Software}
The software which was written is described with a class diagram, seen in Appendix \ref{classdiagram} specifying each distinct class created along with its methods, properties and relationship with other classes, and with a use case diagram showing the general function of the software from the perspective of the user, as seen in Appendix \ref{usecase}. A description of all of the software classes functions is as follows.
\paragraph{MainWindow}
The MainWindow class is the class which is responsible for all of the logic behind the user interface of the main window. It inherits the QMainWindow class; a PyQt type which provides a main application window, and from Ui\_MainWindow, which provides the specification and initialisation of the user interface. The classes main function is handling user input, creating new user interface elements and windows, and allowing the user to view classification results.
\paragraph{DatasetWindow}
The DatasetWindow class is primarily responsible for defining what dataset the user wishes to use, to specify the data type of each column of said dataset. The window also allows users to choose between a testing dataset or k-fold cross validation and specify a number of folds, and to specify the number of runs a stochastic classifier should be run for. This class inherits from QDialog which provides the class with buttons allowing the user to accept or cancel the information they have input. It also inherits from Ui\_DatasetWindow which like Ui\_MainWindow provides the user interface for the window.
\paragraph{UIObject}
UIObject is an interface which is used by other classes which implement a user interface for some window. It is required as when a user interface is inherited by a window class a number of methods are expected to be available in order to create and render the user interface at runtime.
\paragraph{Classifier}
Classifier is an interface which any classification algorithm which is to be used must implement. It has one property, 'stochastic', which determines whether or not the classifier must be run multiple times, and a 'run' method which carries out the classification and returns the results.
\paragraph{QClfSelector}
QClfSelector is a class which inherits from QWidget, the base class of all Qt user interface objects, and who's main function is to act as a widget that can be included within MainWindow multiple times. It provides the functionality of accepting user classifier arrangements and then fetching and running those classifiers, using whatever method has been specified by the user, be it k-fold cross validation or using a test dataset, or running stochastic classifiers multiple times and gathering the results.
\paragraph{QBarChart}
QBarChart is a class which inherits from 'FigureCanvas', a Matplotlib class which interfaces with PyQt in order to create widget which can be displayed within a PyQt application. The class accepts a set of classification results and then constructs a Matplotlib chart which can be displayed in the MainWindow class for the user to view.
\paragraph{ErrorMessage}
The ErrorMessage Class is the class responsible for displaying an error message popup to users, in a standard manner and encapsulates boiler plate code required for creating and showing a message box. The class inherits from QMessageBox which is a PyQt class provides a modal dialog for informing the user.
\subsection{Datasets}
\subsubsection{NSL-KDD}
Within the KDD-NSL dataset, traffic is categorised into either normal network traffic for a military network or into one of the following four attack categories:
\begin{itemize}
\item{Denial of Service (DoS) - Attacker tries to prevent legitimate users from using a service.}
\item{Remote to Local (r2l) - Attacker does not have an account on the victim machine, hence tries to gain access.}
\item{User to Root (u2r) - Attacker has local access to the victim machine and tries to gain super user privileges.}
\item{Probe - Attacker tries to gain information about the target host.}
\end{itemize}
A complete listing of all attacks and their categories can been seen in Table \ref{nslkddattacksamples}, as well as the sample count of each category of attack in Table \ref{nslkddsamples}.
\input{tables/kddattacks}
\input{tables/kddbroad}

The dataset itself is also comprised of 41 features per connection recorded, a detailed listing of each and their type can be seen in Appendix \ref{kddfeatures}.

\subsubsection{UNSW-NB15}
Similarly, within the UNSW-NB15 dataset, as with the NSL-KDD dataset, traffic is also categorised into either normal network traffic for a military network or into one of the following seven attack categories:
\begin{itemize}
	\item{Fuzzers - Attempting to cause a program or network suspended by feeding it the randomly generated data. Analysis 2,677 It contains different attacks of port scan, spam and html files penetrations.}
	\item{Backdoors - A tecnique in which a system security mechanism is bypassed stealthily to access a computer or its data.}
	\item{DoS - A malicious attempt to make a server or a network resource unavailable to users, usually by temporarily interrupting or suspending the services of a host connected to the Internet.}
	\item{Exploits - The attacker knows of a security problem within an operating system or a piece of software and leverages that knowledge by exploiting the vulnerability.}
	\item{Generic - A technique works against all block-ciphers (with a given block and key size), without consideration about the structure of the block-cipher.}
	\item{Reconnaissance - Contains all Strikes that can simulate attacks that gather information.}
	\item{Shellcode - A small piece of code used as the payload in the exploitation of software vulnerability. Worms 174 Attacker replicates itself in order to spread to other computers. Often, it uses a computer network to spread itself, relying on security failures on the target computer to access it.}
\end{itemize}
\input{tables/unswattacks}
	The UNSW-NB15 dataset is comprised of 42 features per connection recorded, a detailed listing of each feature, their type and a description of each can be seen in Appendix \ref{unswfeatures}.
\subsection{Data Pre-processing}
Before classification can take place it is necessary to perform some pre-processing upon both datasets, so that when they are used they are in a form which is most useful for classification, allowing for more accurate, and faster classification. The first form of pre-processing which will take place upon the datasets is in the form of one hot encoding. One hot encoding is a process wherein categorical features of a dataset are converted into a binary representation. This process can be seen in Figure \ref{ohe}, which shows a small snippet from the NSL-KDD dataset before, and after one hot encoding.

\input{figures/onehot}

%REWRITE THIS, EXPLAIN ONEHOT BECAUSE OF AVERAGING NOT DISTANCE
One hot encoding is necessary as many classification algorithms rely on either some kind of internal averaging, or some form of activation function which require numerical input in order to extract meaning from. Simply converting each of these categories from a nominal to a numeric representation is not sufficient as the categories lose meaning when represented by a series of numbers, i.e. when calculating distance from one category to another any number selected will be arbitrary, and the machine learning algorithm will attempt to extract a relationship between the numbers where there is none, therefore reducing accuracy in classification. 

One side effect of one hot encoding is that when processing both the training and testing dataset it is possible that the testing set is missing some categories present within the training set, leading to the algorithms being unable to perform a categorisation. To rectify this the next form of pre-processing which must take place is the insertion of missing features from the training set into the testing set. To do this the training set is simply scanned to find features which are not present in the testing set, and then inserts them into the testing set at the same index thereby making both sets features identical.

The final form of pre-processing to be performed on the dataset is feature scaling. Feature scaling is where a range of data is normalised by scaling it to be within some range, in this case between -1 and 1. The formula used for doing this is seen below in Figure \ref{scale}:
\begin{figure}[H]
	\[ x' = \frac{x - min(x)}{max(x) - min(x)} \] 
\caption{Feature Scaling Formula}
\label{scale}
\end{figure}
where $ x $ is the original value and $ x' $ is the normalised value. Representing a range of values between a normalised range is important as a large number of classification algoithms compute the euclidian distance between two points and will therefore not work correctly if two different featues have wildy different scales. For example if one feature has a large range of numbers and another a very narrow range, the distance will be dependant only on the feature with the larger range, so each feature should be normalised to ensure that each contributes the same amount to the final distance which is computed. Feature scaling also allows classifers which use gradient descent, such as the multi-layer perceptron, to converge much faster allowing for faster collection of results.
\subsection{Classifier Configurations}
The following is a listing of the parameters used in the running of each of the classifiers implemented within the scikit-learn library, with the descriptions of each taken from the official documentation:

\input{tables/classifierconfig}

\subsection{Single Stage Classification}
The first form of classification which will take place is single stage classification. In single stage classification only a single machine learning technique and stage is implemented in order to classify network traffic. The dataset is first read and preprocessed, and the entire set is used in order to train the machine learning classifier. One the classifier has been trained, the classifier will make predictions about the testing set, classifying it into normal traffic or any one of the attack categories for the dataset it was trained upon, e.g. the attacks seen in Table \ref{nslkddattacksamples}.

\subsection{Two Stage Classification}
In second form of classification to take place is two stage classification. In two stage classification, the classification processes is split into two separate stages, the first indentifying normal traffic, and the second identifying abnormal. Before the first stage is trained, the dataset must first be flattened, with all attack types being reduced to simply 'attack', resulting in a set of data labelled only 'normal' and 'attack'. This new dataset is then used to train the first classifier, and then predictions made about the testing set, consisting of only 'normal' and 'attack' classifications. The second classifier is now trained on the original, unflattened dataset with all of the normal traffic removed, resulting in a dataset consisting of only attacks. Once the classifier has been trained, the traffic which was classified as being an attack by the first classifier is then fed into the second classifier, and is then labelled as a specific attack. The total result is then the combination of the traffic identified as normal within the first classifier and the attacks from the second classifier. 

	Performing classification in this manner is expected to give more accurate results than a single stage classifier as it allows each stage to specialise, with the first only needing to differentiate between two classes, 'normal' and 'attack', and the second stage focussed on specific attack types, in theory increasing the overall rate of accurate detection by reducing domain size.

\subsection{Data Collection}
When performing data collection a number of things were done to ensure the greatest consistency within the results gathered, that comparisons between classification methods are valid, and that the results portray the real world performance of such a classifier as closely as possible. The first thing done to ensure validity of results gathered is that classifiers which are stochastic, i.e. are non-deterministic, are run for a total of 30 times and the results are averaged to reduce the amount of randomness within the results and lower the chance that a classifier may perform abnormally well or otherwise.

To best represent the real world performance of the classifiers, k-fold cross validation was used as described by \cite{refaeilzadeh2009cross}. In k-fold cross validation the training dataset is partitioned into $ k $ equal parts, with one part being used as the testing dataset the classifier will make predictions about, and the remaining $ k-1 $ parts used as the training set. This process is repeated $ k $ times with a different fold used each time as the testing set, a diagram of which can been seen in Figure \ref{kfold}. 

\input{figures/kfoldfigure.tex}

The results from all of these folds can then be combined and used to obtain a single result for the performance of the classification method. Using this method for testing the performance of classifier is preferable to the use of a testing set as the all of the data available is used as both training data and testing data, therefore giving a more accurate estimation of how well the classifier will generalize to any independent dataset, i.e. it gives a more accurate idea of how well the classifier will perform in practice.

The k value selected for the k-fold cross validation to be carried out in this investigation is $ k=10 $ as a study carried out by \cite{kohavi1995study} found ten-fold cross validation to be the most effective in producing accurate results. The same k value was used throughout for each classifier, as well as the using the same folds for gathering results without randomising to ensure that results collected were comparable.

Finally the same parameters were used for each of the classifier arrangements, as described in Table \ref{params}, to ensure that results across classifier arrangements were consistent with one and other.

\subsection{Metrics}
The following section describes the metrics which will be extracted from each of the classification results.

Precision is a measure of how many of the classifications made are relevant, the formula for which can be seen below in Figure \ref{precision}:
\begin{figure}[H]
\[ precision = \frac{tp}{tp+fp} \]
\caption{Precision Formula}
\label{precision}
\end{figure}
where $tp$ is the number of true positives, and $fp$ is the number of false positives. For example, if a classifier were to have a precision of 1.0 that would mean that only relevant traffic was classified, there were no false positives. This however does not tell us what percentage of the relative traffic was actually classified. To do this another metric must be used called recall.

Recall, also known as sensitivity, is a measure of what percentage of the total relevant records were classified. The formula for recall can be seen below in Figure \ref{recall}:
\begin{figure}[H]
\[ recall = \frac{tp}{tp+fn} \]
\caption{Recall Formula}
\label{recall}
\end{figure}
where $tp$ is the number of true positives, and $fn$ is the number of false negatives. For example, if a classifier were to have a recall of 1.0, that would mean that every single record of that class had been identified and classified correctly, however even with a recall of 1.0 there may be a large number of false positives which is why it is important to also take a measure of precision. To combine both of these metrics into a single value, f1-score can be used.

f1-Score is a metric which takes both the precision and recall into account, the formula for which can be seen below in Figure \ref{f1}:
\begin{figure}[H]
\[ f_1 = \frac{2tp}{2tp+fp+fn} \]
\caption{f1-Score Formula}
\label{f1}
\end{figure}

where $tp$ is the number of true positives, $fp$ the number of false positives, and $fn$ the number of false negatives. This metric is extremely useful if false positives and false negatives are weighted the same in terms of negative effects. Note that f1-score does not take true negatives into account, however in this case the sheer volume of true negatives for each class would cause a metric taking them into account yield no useful insight.

\section{Evaulation}
\subsection{Software}
Evaluate the software
How many of the function requirements were met
How many of the non functional requirements were met
Is the user interface like it was designed
How does it compare to existing software
\subsection{k-Nearest Neighbours}
\subsection{Multi-layer Perceptron}
\subsection{Support Vector Machine}
\subsection{Single-Stage Classifiers Performance}
\subsection{Two-stage Classifiers Performance}
in knn-knn the second stage for unsw results in the same normals but removing normals mean that it cannot detect attacks as easily. In the svm-svm the normal has a lower number of true positives than the single stage, increasing false postivies within attacks.

\section{Conclusion}
\subsection{Research Questions}
\subsubsection*{What is the rate of accurate detection and classification of network intrusions by single stage machine learning classification methods?}
\subsubsection*{What is the rate of accurate detection and classification of network intrusions by two stage machine learning classification methods?}
\subsubsection*{Which method of classification i.e. single stage or two stage, is more accurate and by what amount?}
\subsubsection*{Which configuration of algorithms in the two stage classifier produces the most accurate results?}

\subsection{Has the Project met it's Aims and Objectives?}
\subsection{Reflection}
\subsection{Learning Outcomes}
\subsubsection*{LO1 - Manage a substantial individual project, including planning, documentation and control}
\subsubsection*{LO2 - Construct a focussed problem statement and conduct a suitable investigation, including literature or technology review, into the context of that problem}
\subsubsection*{LO3 - Demonstrate professional competence by applying appropriate theory and practice to the analysis, design, implementation and evaluation of a non-trivial set of deliverables}
\subsubsection*{LO4 - Show a capacity for self-appraisal by analysing the strengths and weaknesses of the project process and outcomes with reference to the initial objectives and to the work of others}
\subsection{Further Research}

\newpage
	\printbibliography
\newpage

\begin{appendices}
\section{Project Overview}

%\input{ipo}
\includegraphics[page=1, width=\textwidth]{figures/ipo}
\newpage
\includegraphics[page=2, width=\textwidth]{figures/ipo}
\newpage
\includegraphics[page=3, width=\textwidth]{figures/ipo}
\newpage

\begin{subappendices}
\subsection{Project Timeline}

\begin{figure}[H]
	\centering
	\includegraphics[width=1.31\textwidth,angle=90]{figures/Ganttchart}
	\caption{Project Timeline Gantt Chart}
	\label{gantt}
\end{figure}

\end{subappendices}

\newpage

\section{Second Formal Review Output}
\begin{center}
\includegraphics[page=1, width=\textwidth, center]{figures/interim}
\newpage
\includegraphics[page=2, width=\textwidth, center]{figures/interim}
\end{center}
\newpage

\section{Diary Sheets}

\begin{longtable}{@{}cc@{}}
\includegraphics[page=1, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=2, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=3, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=4, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=5, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=6, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=7, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=8, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=9, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=10, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=11, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=12, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=13, width=0.5\textwidth]{figures/diaries} &
\includegraphics[page=14, width=0.5\textwidth]{figures/diaries} \\
\includegraphics[page=15, width=0.5\textwidth]{figures/diaries} &
\end{longtable}


\newpage

\section{MoSCoW Analysis}
\input{tables/moscow}

\newpage

\section{Class Diagram} \label{classdiagram}
\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth, center]{figures/classuml.pdf}}
\caption{UML Class Diagram}
\end{figure}

\newpage

\section{Use Case Diagram} \label{usecase}
\begin{figure}[H]
	\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth, center]{figures/usecase.pdf}}
\caption{UML Use Case Diagram}
\end{figure}

\newpage

\section{NSL-KDD Features} \label{kddfeatures}
\input{tables/nslkddfeatures}

\newpage

\section{UNSW-NB15 Features} \label{unswfeatures}

\input{tables/unswdescription}

\newpage

\section{NSL-KDD Precision}
\input{tables/nslprecision}
\input{plots/precision}

\newpage


\section{NSL-KDD Recall}
\input{tables/nslrecall}
\input{plots/recall}

\newpage

\section{NSL-KDD F1-Score}
\input{tables/nslf1score}
\input{plots/f1score}

\newpage


\section{UNSW-NB15 Precision}
\input{tables/UNSWpecision}
\input{plots/precisionUNSW}

\newpage

\section{UNSW-NB15 Recall}
\input{tables/UNSWrecall}
\input{plots/recallUNSW}

\newpage

\section{UNSW-NB15 f1-Score}
\input{tables/UNSWf1score.tex}
\input{plots/f1scoreUNSW}

\newpage


\end{appendices}

\end{document}
