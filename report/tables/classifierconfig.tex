% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{longtable}{@{}p{2cm}llp{5.5cm}@{}}
\caption{Classifier Parameters} \\
\label{params} \\
\toprule
 & Parameter & Value & Description \\ \midrule
k-Nearest Neighbour & algorithm & 'kdtree' & Algorithm used to compute the nearest neighbors. \\
 & weights & 'uniform' & Uniform weights. All points in each neighborhood are weighted equally. \\
 & leaf\_size & 40 & Leaf size used for the KDTree. \\
 & p & 2 & Power parameter for the Minkowski metric. \\
 & n\_neighbors & 3 & The number of nearest neighbours to search for, i.e. $k$. \\
Multi-Layer Perceptron & hidden\_layer\_sizes & 100 & The number of neurons within the hidden layer. \\
 & activation & 'relu' & Activation function for the hidden layer, rectified linear unit function, returns \newline $ f(x) = max(0, x) $. \\
 & solver & 'adam' & The solver for weight optimization, ‘adam’ refers to a stochastic gradient-based optimizer proposed by \cite{kingma2014adam}. \\
 & learning\_rate & 'invscaling' & The learning schedule, ‘invscaling’ gradually decreases the learning rate at each time step. \\
Support Vector Machine & kernel & 'rbf' & Specifies the kernel type to be used in the algorithm, Radial Basis Function kernel. \\
 & gamma & $ 1e-1 $ & How much influence a single training example has. The larger gamma is, the closer other examples must be to be affected. \\
 & C & 10 & Trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. \\ \bottomrule
\end{longtable}
